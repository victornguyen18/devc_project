{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x7P-tm-0R2C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/var/www/html/devc_project/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imutils\n",
    "import dlib\n",
    "import skimage\n",
    "import math\n",
    "import pytesseract\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from shutil import copy\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "#from face_verification.api import *\n",
    "#from face_verification.utils import *\n",
    "#from .utils import read_image, BGR2RGB, BGR2Gray, crop_face\n",
    "#from .facenet import triplet_loss\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cccd = \"../image/cccd/\"\n",
    "path_facial_verification = \"../image/facial_verification/\"\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CznqBtx2L_ZM"
   },
   "source": [
    "#Utilities For Facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyLdC8mHMGdl"
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    \"\"\" function to read single image at the given path\n",
    "        note: the loaded image is in B G R format\n",
    "    \"\"\"\n",
    "    return cv.imread(path)\n",
    "\n",
    "\n",
    "def BGR2RGB(image):\n",
    "    \"\"\" function to transform image from BGR into RBG format \"\"\"\n",
    "    return cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def BGR2Gray(image):\n",
    "    \"\"\" function to transofrm image from BGR into Gray format \"\"\"\n",
    "    return cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def show_image(image, img_format='RGB', figsize=(8, 6)):\n",
    "    \"\"\" function to show image \"\"\"\n",
    "    if img_format == 'RGB' or img_format == 'Gray':\n",
    "        pass\n",
    "    elif img_format == 'BGR':\n",
    "        image = BGR2RGB(image)\n",
    "    else:\n",
    "        raise ValueError('format should be \"RGB\", \"BGR\" or \"Gray\"')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if format == 'Gray':\n",
    "        ax.imshow(image, format='gray')\n",
    "    else:\n",
    "        ax.imshow(image)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def denote_face(image, face):\n",
    "    \"\"\" function to denote location of face on image \"\"\"\n",
    "    img = image.copy()\n",
    "    for (x, y, w, h) in face:\n",
    "        cv.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def crop_face(image, face, scale_factor=1.0, target_size=(128, 128)):\n",
    "    \"\"\" crop face at the given positons and resize to target size \"\"\"\n",
    "    rows, columns, channels = image.shape\n",
    "    x, y, w, h = face[0]\n",
    "    mid_x = x + w // 2\n",
    "    mid_y = y + h // 2\n",
    "\n",
    "    # calculate the new vertices\n",
    "    x_new = mid_x - int(w // 2 * scale_factor)\n",
    "    y_new = mid_y - int(h // 2 * scale_factor)\n",
    "    w_new = int(w * scale_factor)\n",
    "    h_new = int(h * scale_factor)\n",
    "\n",
    "    # validate the new vertices\n",
    "    left_x = max(0, x_new)\n",
    "    left_y = max(0, y_new)\n",
    "    right_x = min(columns, x_new + w_new)\n",
    "    right_y = min(rows, y_new + h_new)\n",
    "\n",
    "    # crop and resize the facial area\n",
    "    cropped = image[left_y:right_y, left_x:right_x, :]\n",
    "    resized = cv.resize(cropped, dsize=target_size, interpolation=cv.INTER_LINEAR)\n",
    "\n",
    "    return resized\n",
    "\n",
    "# Using this function after cropped images\n",
    "def crop_using_facial_landmark(cv2readimage):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('pretrained_model/shape_predictor_68_face_landmarks.dat')\n",
    "    rect = detector(cv2readimage)[0]\n",
    "    sp = predictor(cv2readimage, rect)\n",
    "    landmarks = np.array([[p.x, p.y] for p in sp.parts()])\n",
    "    outline = landmarks[[*range(17), *range(26,16,-1)]]\n",
    "    Y, X = skimage.draw.polygon(outline[:,1], outline[:,0])\n",
    "    cropped_img_landmark = np.zeros(cv2readimage.shape, dtype=np.uint8)\n",
    "    cropped_img_landmark[Y, X] = cv2readimage[Y, X]\n",
    "    return cropped_img_landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjwRRoe-Klyg"
   },
   "source": [
    "#API For Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cvYoJ9QMWaU",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xml = 'pretrained_model/haarcascade_frontalface_default.xml'\n",
    "def cascade_detector(image, xml=xml, scale_factor=1.3, min_neighbors=5):\n",
    "    \"\"\" implement Haar or LBP Feature-based Cascade Classifiers from OpenCV\n",
    "        change the xml to specify Haar or LBP Cascade detector\n",
    "        note: the image format should be BGR, instead of RGB\n",
    "    \"\"\"\n",
    "    face_detector = cv.CascadeClassifier(xml)\n",
    "    gray_img = BGR2Gray(image)\n",
    "    faces = face_detector.detectMultiScale(gray_img, scaleFactor=scale_factor, minNeighbors=min_neighbors)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        raise ValueError('Error, there is no faces.')\n",
    "    elif len(faces) > 1:\n",
    "        raise ValueError('Error, multiple faces are found.')\n",
    "\n",
    "    return faces\n",
    "\n",
    "\n",
    "class face_verify(object):\n",
    "    \"\"\" class for face verification \"\"\"\n",
    "    def __init__(self, \n",
    "                 path='pretrained_model/facenet-margin-04-final.h5', \n",
    "                 xml='pretrained_model/haarcascade_frontalface_default.xml'):\n",
    "        \"\"\" initialize the face verification api \"\"\"\n",
    "        self.path = path\n",
    "        self.xml = xml\n",
    "        self.model = None\n",
    "        \n",
    "    def get_distance(self, path1, path2):\n",
    "        \"\"\" get the distance between two images from path1 and path2 \"\"\"\n",
    "        if self.model is None:\n",
    "            self._load_model()\n",
    "            \n",
    "        # pre-process the images\n",
    "        img1 = self._process_image(path1)\n",
    "        img2 = self._process_image(path2)\n",
    "        \n",
    "        # make predictions\n",
    "        imgs = np.array([img1, img2])\n",
    "        predictions = self.model.predict(imgs)\n",
    "        \n",
    "        pred1 = predictions[0]\n",
    "        pred2 = predictions[1]\n",
    "        \n",
    "        # calculate the Euclidean distance\n",
    "        distance = np.sqrt(np.sum(np.square(pred1 - pred2)))\n",
    "        \n",
    "        return img1, img2, distance\n",
    "    \n",
    "    def verify(self, path1, path2, threshold=0.2):\n",
    "        \"\"\" verify whether or not images from path1 and path2 are same person \"\"\"\n",
    "        img1, img2, distance = self.get_distance(path1, path2)\n",
    "        \n",
    "        if distance < threshold:\n",
    "            return img1, img2, True\n",
    "        \n",
    "        return img1, img2, False\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\" load the pre-defined cnn model for face verification \"\"\"\n",
    "        self.model = load_model(self.path, custom_objects={'tf': tf})\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def _process_image(self, path):\n",
    "        \"\"\" read and pre-process the images \"\"\"\n",
    "        image = read_image(path)\n",
    "        \n",
    "        # frontal face detection\n",
    "        faces = cascade_detector(image, xml=self.xml, scale_factor=1.3, min_neighbors=5)\n",
    "        \n",
    "        # crop frontal face areas\n",
    "        crop = crop_face(image, faces, scale_factor=1.3, target_size=(256, 256))\n",
    "        crop_landmark = crop_using_facial_landmark(crop)\n",
    "        crop_landmark_resize = imutils.resize(crop_landmark, 96, 96)\n",
    "        crop_rgb = BGR2RGB(crop_landmark_resize)\n",
    "        # crop_array = np.array(crop_rgb, dtype=K.floatx()) / 255.0\n",
    "        \n",
    "        return crop_rgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBncVenBMxSN"
   },
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../image/\"\n",
    "image_path = path + 'cccd/DTN_CCCD_Shot2.jpg'\n",
    "# image_path = path + 'cccd/DTN_CCCD_Shot_Error_5.jpg'\n",
    "# image_path = path + 'cccd/NDMT_CCCD.jpg'\n",
    "# image_path = path + 'cccd/NDMT_CCCD_Error.jpg'\n",
    "# image_path = path + 'cccd/NDMT_CCCD_Error_3.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cccd_path = path_cccd + 'NDMT_CCCD.jpg'\n",
    "img = cv.imread(image_path, cv.IMREAD_COLOR)\n",
    "img_scale = imutils.resize(img, height=500)\n",
    "# show_image(img_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6sOBcMGPKLS"
   },
   "outputs": [],
   "source": [
    "haar_xml = 'pretrained_model/haarcascade_frontalface_default.xml'\n",
    "# gray = cv2.cvtColor(img_scale, cv2.COLOR_BGR2GRAY)\n",
    "try:\n",
    "    faces = cascade_detector(img_scale, xml=haar_xml, scale_factor=1.3, min_neighbors=5)\n",
    "except:\n",
    "    print('oops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "2bZKp7vTSNEr",
    "outputId": "d8ccbd75-fa72-4838-f678-486d12f7bbd0"
   },
   "outputs": [],
   "source": [
    "rec_images = denote_face(img_scale, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "-s6gro03TBBR",
    "outputId": "eb92f948-68ad-440f-cb4e-ecaa0defbc3c"
   },
   "outputs": [],
   "source": [
    "cropped = crop_face(img_scale, faces, scale_factor=1.3, target_size=(256, 256))\n",
    "show_image(BGR2RGB(cropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "TTrUZkWQQgkn",
    "outputId": "92993b42-6364-490a-911a-8fcb8ad958b4"
   },
   "outputs": [],
   "source": [
    "portrait_path = path_facial_verification + 'NPXT_Portrait.jpg'\n",
    "portrait = cv.imread(portrait_path, cv.IMREAD_COLOR)\n",
    "portrait_scale = imutils.resize(portrait, height = 500)\n",
    "show_image(portrait_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7jspWo3Q6Y3"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    faces_portrait = cascade_detector(portrait_scale, xml=haar_xml, scale_factor=1.1, min_neighbors=9)\n",
    "except:\n",
    "    print('oops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "gIPw_9LOQ_bO",
    "outputId": "cdc210c0-2f73-4bb9-d1d2-903972b7a4d9"
   },
   "outputs": [],
   "source": [
    "rec_portrait_images = denote_face(portrait_scale, faces_portrait)\n",
    "show_image(BGR2RGB(rec_portrait_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "PKjwC-OvRHL4",
    "outputId": "aed94a32-58bc-4f92-e73c-063dd52de4c0"
   },
   "outputs": [],
   "source": [
    "cropped_portrait = crop_face(portrait_scale, faces_portrait, scale_factor=1.3, target_size=(256, 256))\n",
    "show_image(BGR2RGB(cropped_portrait))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mkon3sqRSDj"
   },
   "outputs": [],
   "source": [
    "# define path to pre-trained model and haar xml\n",
    "model_path = 'pretrained_model/facenet-margin-04-final.h5'\n",
    "\n",
    "# intialize face verification model\n",
    "face_model = face_verify(path=model_path, xml=haar_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vZzM_RHAYeyZ",
    "outputId": "7d199c7d-8116-4d81-8477-b177601c916a"
   },
   "outputs": [],
   "source": [
    "cv.imwrite('scale_cccd.jpg', img_scale)\n",
    "cv.imwrite('portrait_scale.jpg', portrait_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USjV7tkuReu6"
   },
   "outputs": [],
   "source": [
    "img1, img2, distance = face_model.get_distance('scale_cccd.jpg', 'portrait_scale.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "41Hxvl_xTd6W",
    "outputId": "cec7c916-1d73-469f-9a9f-dd5bfc4e56ed"
   },
   "outputs": [],
   "source": [
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "colab_type": "code",
    "id": "L4TzpRyeUCD9",
    "outputId": "b85570d3-6e60-4a4d-eb44-55a9383c53f5"
   },
   "outputs": [],
   "source": [
    "# show_image(img1)\n",
    "show_image(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Facial_Verification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
